{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc23c14",
   "metadata": {},
   "source": [
    "### Building a RAG System with LangChain and FAISS \n",
    "Introduction to RAG (Retrieval-Augmented Generation)\n",
    "RAG combines the power of retrieval systems with generative AI models. Instead of relying solely on the model's training data, RAG:\n",
    "\n",
    "1. Retrieves relevant documents from a knowledge base\n",
    "2. Uses these documents as context for the LLM\n",
    "3. Generates responses based on both the retrieved context and the model's knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c4df4",
   "metadata": {},
   "source": [
    "### FAISS \n",
    "https://github.com/facebookresearch/faiss\n",
    "\n",
    "FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "Key advantages:\n",
    "1. Extremely fast similarity search\n",
    "2. Memory efficient\n",
    "3. Supports GPU acceleration\n",
    "4. Can handle millions of vectors\n",
    "\n",
    "How it works:\n",
    "- Indexes vectors for fast nearest neighbor search\n",
    "- Returns most similar vectors based on distance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0704443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Ingestion And Processing\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Artificial Intelligence (AI) is the simulation of human intelligence in machines.\n",
    "        These systems are designed to think like humans and mimic their actions.\n",
    "        AI can be categorized into narrow AI and general AI.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"AI Introduction\", \"page\": 1, \"topic\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Machine Learning is a subset of AI that enables systems to learn from data.\n",
    "        Instead of being explicitly programmed, ML algorithms find patterns in data.\n",
    "        Common types include supervised, unsupervised, and reinforcement learning.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ML Basics\", \"page\": 1, \"topic\": \"ML\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Deep Learning is a subset of machine learning based on artificial neural networks.\n",
    "        It uses multiple layers to progressively extract higher-level features from raw input.\n",
    "        Deep learning has revolutionized computer vision, NLP, and speech recognition.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"Deep Learning\", \"page\": 1, \"topic\": \"DL\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\n",
    "        It combines computational linguistics with machine learning and deep learning models.\n",
    "        Applications include chatbots, translation, sentiment analysis, and text summarization.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"NLP Overview\", \"page\": 1, \"topic\": \"NLP\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(sample_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "## text splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "\n",
    "## split the documents into chunks\n",
    "chunks = text_splitter.split_documents(sample_documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(sample_documents)} documents\")\n",
    "print(\"\\nExample chunk:\")\n",
    "print(f\"Content: {chunks[0].page_content}\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae73d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings with the latest model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f236630",
   "metadata": {},
   "source": [
    "### Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd718510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de19fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarity Search \n",
    "query=\"What is deep learning\"\n",
    "\n",
    "results=vectorstore.similarity_search(query,k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce89965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 similar chunks:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efab28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity Search with score\n",
    "results_with_scores=vectorstore.similarity_search_with_score(query,k=3)\n",
    "\n",
    "print(\"\\n\\nSimilarity search with scores:\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"\\nScore: {score:.3f}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search with metadata filtering\n",
    "filter_dict={\"topic\":\"ML\"}\n",
    "filtered_results=vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter=filter_dict\n",
    ")\n",
    "print(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabac4f3",
   "metadata": {},
   "source": [
    "### Build RAG Chain With LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e742716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb15e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple RAG Chain with LCEL\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab7a922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic retriever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb2c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Format documents for the prompt\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for insertion into prompt\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"Document {i+1} (Source: {source}):\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b687f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "simple_rag_chain=(\n",
    "    {\"context\":retriever | format_docs,\"question\":RunnablePassthrough() }\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "simple_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb9f0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conversational RAg Chain\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer questions.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48908c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag():\n",
    "    \"\"\"Create a conversational RAG chain with memory\"\"\"\n",
    "    return (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "        )\n",
    "        | conversational_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conversational_rag = create_conversational_rag()\n",
    "conversational_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef906449",
   "metadata": {},
   "outputs": [],
   "source": [
    "### streaming RAG chain\n",
    "streaming_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(\"Modern RAG chains created successfully!\")\n",
    "print(\"Available chains:\")\n",
    "print(\"- simple_rag_chain: Basic Q&A\")\n",
    "print(\"- conversational_rag: Maintains conversation history\")\n",
    "print(\"- streaming_rag_chain: Supports token streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0862ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for different chain types\n",
    "def test_rag_chains(question: str):\n",
    "    \"\"\"Test all RAG chain variants\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Simple RAG\n",
    "    print(\"\\n1. Simple RAG Chain:\")\n",
    "    answer = simple_rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    print(\"\\n2. Streaming RAG:\")\n",
    "    print(\"Answer: \", end=\"\", flush=True)\n",
    "    for chunk in streaming_rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee043810",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rag_chains(\"What is the difference between AI and machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8093c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"What is the difference between AI and Machine Learning?\",\n",
    "    \"Explain deep learning in simple terms\",\n",
    "    \"How does NLP work?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    test_rag_chains(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversational example\n",
    "print(\"\\n3. Conversational RAG Example:\")\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "q1 = \"What is machine learning?\"\n",
    "a1 = conversational_rag.invoke({\n",
    "    \"input\": q1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q1: {q1}\")\n",
    "print(f\"A1: {a1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "358ff56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=q1),\n",
    "    AIMessage(content=a1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287146a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "q2 = \"How is it different from traditional programming?\"\n",
    "a2 = conversational_rag.invoke({\n",
    "    \"input\": q2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nQ2: {q2}\")\n",
    "print(f\"A2: {a2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
