{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752dfe68",
   "metadata": {},
   "source": [
    "Steps to setup a Project\n",
    "\n",
    "Create Virtual Environment and swith to virtual environment\n",
    "pip install uv\n",
    "uv init\n",
    "uv add ipykernel, langchain, langchain-community, dotenv, langchain_openai, chromadb\n",
    "Create a API KEY from OPENAI and add in .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84958c9",
   "metadata": {},
   "source": [
    "### Building a RAG System with LangChain and ChromaDB\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the capabilities of large language models with external knowledge retrieval. This notebook will walk you through building a complete RAG system using:\n",
    "\n",
    "- LangChain: A framework for developing applications powered by language models\n",
    "- ChromaDB: An open-source vector database for storing and retrieving embeddings\n",
    "- OpenAI: For embeddings and language model (you can substitute with other providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa88e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f5969",
   "metadata": {},
   "source": [
    "# RAG Architecture Overview\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) Architecture:\n",
    "\n",
    "1. Document Loading: Load documents from various sources\n",
    "2. Document Splitting: Break documents into smaller chunks\n",
    "3. Embedding Generation: Convert chunks into vector representations\n",
    "4. Vector Storage: Store embeddings in ChromaDB\n",
    "5. Query Processing: Convert user query to embedding\n",
    "6. Similarity Search: Find relevant chunks from vector store\n",
    "7. Context Augmentation: Combine retrieved chunks with query\n",
    "8. Response Generation: LLM generates answer using context\n",
    "\n",
    "Benefits of RAG:\n",
    "- Reduces hallucinations\n",
    "- Provides up-to-date information\n",
    "- Allows citing sources\n",
    "- Works with domain-specific knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09810b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample Data\n",
    "\n",
    "## create sample documents\n",
    "sample_docs = [\n",
    "    \"\"\"\n",
    "    Machine Learning Fundamentals\n",
    "    \n",
    "    Machine learning is a subset of artificial intelligence that enables systems to learn \n",
    "    and improve from experience without being explicitly programmed. There are three main \n",
    "    types of machine learning: supervised learning, unsupervised learning, and reinforcement \n",
    "    learning. Supervised learning uses labeled data to train models, while unsupervised \n",
    "    learning finds patterns in unlabeled data. Reinforcement learning learns through \n",
    "    interaction with an environment using rewards and penalties.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Deep Learning and Neural Networks\n",
    "    \n",
    "    Deep learning is a subset of machine learning based on artificial neural networks. \n",
    "    These networks are inspired by the human brain and consist of layers of interconnected \n",
    "    nodes. Deep learning has revolutionized fields like computer vision, natural language \n",
    "    processing, and speech recognition. Convolutional Neural Networks (CNNs) are particularly \n",
    "    effective for image processing, while Recurrent Neural Networks (RNNs) and Transformers \n",
    "    excel at sequential data processing.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Natural Language Processing (NLP)\n",
    "    \n",
    "    NLP is a field of AI that focuses on the interaction between computers and human language. \n",
    "    Key tasks in NLP include text classification, named entity recognition, sentiment analysis, \n",
    "    machine translation, and question answering. Modern NLP heavily relies on transformer \n",
    "    architectures like BERT, GPT, and T5. These models use attention mechanisms to understand \n",
    "    context and relationships between words in text.\n",
    "    \"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaefd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save sample documents to files\n",
    "import tempfile\n",
    "temp_dir=tempfile.mkdtemp()\n",
    "\n",
    "for i,doc in enumerate(sample_docs):\n",
    "    with open(f\"{temp_dir}/doc_{i}.txt\",\"w\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "print(f\"Sample document create in : {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Document Loading\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "\n",
    "# Load documents from directory\n",
    "loader = DirectoryLoader(\n",
    "    temp_dir, \n",
    "    glob=\"*.txt\", \n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'}\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nFirst document preview:\")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f24b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Documents Spliting\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Maximum size of each chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\" \"]  # Hierarchy of separators\n",
    ")\n",
    "chunks=text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\nChunk example:\")\n",
    "print(f\"Content: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intilialize the ChromaDB Vector Store And Stores the chunks in Vector Representation \n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "## Create a Chromdb vector store\n",
    "persist_directory=\"./chroma_db\"\n",
    "\n",
    "## Initialize Chromadb with Open AI embeddings\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"rag_collection\"\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore._collection.count()} vectors\")\n",
    "print(f\"Persisted to: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99635b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Similarity Search\n",
    "\n",
    "query=\"What are the types of machine learning?\"\n",
    "\n",
    "similar_docs=vectorstore.similarity_search(query,k=3)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nTop {len(similar_docs)} similar chunks:\")\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8907f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Similarity Search With Scores\n",
    "\n",
    "results_scores=vectorstore.similarity_search_with_score(query,k=3)\n",
    "results_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7e1bd",
   "metadata": {},
   "source": [
    "#### Understanding Similarity Scores\n",
    "The similarity score represents how closely related a document chunk is to your query. The scoring depends on the distance metric used:\n",
    "\n",
    "ChromaDB default: Uses L2 distance (Euclidean distance)\n",
    "\n",
    "- Lower scores = MORE similar (closer in vector space)\n",
    "- Score of 0 = identical vectors\n",
    "- Typical range: 0 to 2 (but can be higher)\n",
    "\n",
    "\n",
    "Cosine similarity (if configured):\n",
    "\n",
    "- Higher scores = MORE similar\n",
    "- Range: -1 to 1 (1 being identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46509b2",
   "metadata": {},
   "source": [
    "#### Initialize LLM, RAG Chain, Prompt Template,Query the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "test_response=llm.invoke(\"What is Large Language Models\")\n",
    "test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33454aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modern RAG Chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "\n",
    "## Create a prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt=\"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "## Convert vector store to retriever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_kwarg={\"k\":3} ## Retrieve top 3 relevant chunks\n",
    ")\n",
    "\n",
    "### Create a document chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "### Create The Final RAG Chain\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e686256",
   "metadata": {},
   "source": [
    "This chain ( create_stuff_documents_chain ):\n",
    "\n",
    "- Takes retrieved documents\n",
    "- \"Stuffs\" them into the prompt's {context} placeholder\n",
    "- Sends the complete prompt to the LLM\n",
    "- Returns the LLM's response\n",
    "\n",
    "\n",
    "#### What is create_retrieval_chain?\n",
    "create_retrieval_chain is a function that combines a retriever (which fetches relevant documents) with a document chain (which processes those documents with an LLM) to create a complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain.invoke({\"input\":\"What is Deep Learning\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the modern RAG system\n",
    "def query_rag_modern(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Using create_retrieval_chain approach\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    for i, doc in enumerate(result['context']):\n",
    "        print(f\"\\n--- Source {i+1} ---\")\n",
    "        print(doc.page_content[:200] + \"...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"What is deep learning and how does it relate to neural networks?\",\n",
    "    \"What are CNNs best used for?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = query_rag_modern(question)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3fcd8",
   "metadata": {},
   "source": [
    "### Create RAG Chain Alternative - Using LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a68ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even more flexible approach using LCEL\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "\n",
    "# Create a custom prompt\n",
    "custom_prompt = ChatPromptTemplate.from_template(\"\"\"Use the following context to answer the question. \n",
    "If you don't know the answer based on the context, say you don't know.\n",
    "Provide specific details from the context to support your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "custom_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "32b94813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format the output documents for the prompt\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the chain ussing LCEL\n",
    "\n",
    "rag_chain_lcel=(\n",
    "    { \n",
    "        \"context\":retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "     }\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_lcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain_lcel.invoke(\"What is Deep Learning\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a99bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using the LCEL approach - Fixed version\n",
    "def query_rag_lcel(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Method 1: Pass string directly (when using RunnablePassthrough)\n",
    "    answer = rag_chain_lcel.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    \n",
    "    # Get source documents separately if needed\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\n--- Source {i+1} ---\")\n",
    "        print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1194d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LCEL chain\n",
    "print(\"Testing LCEL Chain:\")\n",
    "query_rag_lcel(\"What are the key concepts in reinforcement learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca58818",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rag_lcel(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741a4f0",
   "metadata": {},
   "source": [
    "### Add New Documents To Existing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90b35a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new documents to the existing vector store\n",
    "new_document = \"\"\"\n",
    "Reinforcement Learning in Detail\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning where an agent learns to make \n",
    "decisions by interacting with an environment. The agent receives rewards or penalties \n",
    "based on its actions and learns to maximize cumulative reward over time. Key concepts \n",
    "in RL include: states, actions, rewards, policies, and value functions. Popular RL \n",
    "algorithms include Q-learning, Deep Q-Networks (DQN), Policy Gradient methods, and \n",
    "Actor-Critic methods. RL has been successfully applied to game playing (like AlphaGo), \n",
    "robotics, and autonomous systems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0db991e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc=Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\"source\": \"manual_addition\", \"topic\": \"reinforcement_learning\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the documents\n",
    "new_chunks=text_splitter.split_documents([new_doc])\n",
    "new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add new documents to vectorstore\n",
    "vectorstore.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Added {len(new_chunks)} new chunks to the vector store\")\n",
    "print(f\"Total vectors now: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## query with the updated vector\n",
    "new_question=\"What are the keys concepts in reinforcement learning\"\n",
    "result=query_rag_lcel(new_question)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779346a",
   "metadata": {},
   "source": [
    "### Advanced Rag Techniques- Conversational Memory\n",
    "Understanding Conversational Memory in RAG\n",
    "Conversational memory enables RAG systems to maintain context across multiple interactions. This is crucial for:\n",
    "\n",
    "Follow-up questions that reference previous answers\n",
    "Pronoun resolution (e.g., \"it\", \"they\", \"that\")\n",
    "Context-dependent queries that build on prior discussion\n",
    "Natural dialogue flow where users don't repeat context\n",
    "\n",
    "Key Challenge:\n",
    "Traditional RAG retrieves documents based only on the current query, missing important context from the conversation. For example:\n",
    "\n",
    "User: \"Tell me about Python\"\n",
    "Bot: explains Python programming language\n",
    "User: \"What are its main libraries?\" ‚Üê \"its\" refers to Python, but retriever doesn't know this\n",
    "\n",
    "Solution:\n",
    "The modern approach uses a two-step process:\n",
    "\n",
    "Query Reformulation: Transform context-dependent questions into standalone queries\n",
    "Context-Aware Retrieval: Use the reformulated query to fetch relevant documents\n",
    "\n",
    "- create_history_aware_retriever: Makes the retriever understand conversation context\n",
    "- MessagesPlaceholder: Placeholder for chat history in prompts\n",
    "- HumanMessage/AIMessage: Structured message types for conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03cafd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "## create a prompt that includes the chat history\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create history aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28231b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document chain with history\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create conversational RAG chain\n",
    "conversational_rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, \n",
    "    question_answer_chain\n",
    ")\n",
    "print(\"Conversational RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "# First question\n",
    "result1 = conversational_rag_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"What is machine learning?\"\n",
    "})\n",
    "print(f\"Q: What is machine learning?\")\n",
    "print(f\"A: {result1['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08519a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([\n",
    "    HumanMessage(content=\"What is machine learning\"),\n",
    "    AIMessage(content=result1['answer'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question\n",
    "result2 = conversational_rag_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"What are its main types?\"  # Refers to ML from previous question\n",
    "})\n",
    "result2['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
